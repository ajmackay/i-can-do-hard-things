[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Series\n\nWoRkflow Tips: Ways to streamline your R workflow.\n\n\nGgplot2 series: Everything I know about ggplot2.\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nText Pre-Processing\n\n\n\n\n\n\n\nNLP\n\n\n\n\n\n\n\n\n\n\n\nMar 26, 2024\n\n\n\n\n\n\n\n\nText Vectorisation\n\n\n\n\n\n\n\nNLP\n\n\n\n\n\n\n\n\n\n\n\nMar 26, 2024\n\n\n\n\n\n\n\n\nTopic Modeling\n\n\n\n\n\n\n\nNLP\n\n\n\n\n\n\n\n\n\n\n\nMar 26, 2024\n\n\n\n\n\n\n\n\nTopic Modeling in NLP Outline\n\n\n\n\n\n\n\nNLP\n\n\n\n\n\n\n\n\n\n\n\nJan 19, 2024\n\n\n\n\n\n\n\n\nIntroduction to the Series\n\n\n\n\n\n\n\nworkflow\n\n\n\n\n\n\n\n\n\n\n\nJan 19, 2024\n\n\n\n\n\n\n\n\nStructuring your R Project\n\n\n\n\n\n\n\nworkflow\n\n\n\n\n\n\n\n\n\n\n\nJan 19, 2024\n\n\n\n\n\n\n\n\nCopying Windows File Paths\n\n\n\n\n\n\n\nworkflow\n\n\n\n\n\n\n\n\n\n\n\nJan 12, 2024\n\n\n\n\n\n\n\n\nNamed Lists from Group Maps\n\n\n\n\n\n\n\niteration\n\n\npurrr\n\n\nmap\n\n\n\n\n\n\n\n\n\n\n\nDec 12, 2023\n\n\n\n\n\n\n\n\nAccurately Viewing and Saving Plots using Camcorder\n\n\n\n\n\n\n\nggplot2\n\n\n\n\n\n\n\n\n\n\n\nNov 12, 2023\n\n\n\n\n\n\n\n\nSuper Helpful Very Little Tips and Tricks\n\n\n\n\n\n\n\nggplot\n\n\ndataviz\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\n\n\n\n\n\n\nCreating a ggplot2 theme\n\n\n\n\n\n\n\nggplot\n\n\ndataviz\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\n\n\n\n\n\n\nHighlighting In ggplot\n\n\n\n\n\n\n\nggplot\n\n\ndataviz\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\n\n\n\n\n\n\nCreating a ggplot2 theme\n\n\n\n\n\n\n\nggplot\n\n\ndataviz\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2023\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2023\n\n\n\n\n\n\n\n\nWoRkflow\n\n\n\n\n\n\n\nworkflow\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2023\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nAug 27, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Figuring it Out by Aaron",
    "section": "",
    "text": "These are some things I know about R and Data Science - test"
  },
  {
    "objectID": "posts/r-workflow/index.html",
    "href": "posts/r-workflow/index.html",
    "title": "WoRkflow",
    "section": "",
    "text": "Getting the right workflow in R is hard.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/Quickly highlighting ggplot columns/post.html",
    "href": "posts/Quickly highlighting ggplot columns/post.html",
    "title": "Highlighting In ggplot",
    "section": "",
    "text": "Highlighting specific columns in ggplot is a useful\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/Quarto Tips/index.html",
    "href": "posts/Quarto Tips/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "Use knitr::opts_chunk$set(echo = , warning = , message = ) to set a default chunk option\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/Capturing Warnings/index.html",
    "href": "posts/Capturing Warnings/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "Capturing Warnings is like super useful and stuff.\n\nwithCallingHandlers()\ntryCatch()\n\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/Cumulative vs Per Period Plots/index.html",
    "href": "posts/Cumulative vs Per Period Plots/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "When would you use a cumulative plot vs a per period one (hint, cumulative plots tend to be better for spotting trends etc.)\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/Quickest way to add text to ggplot/index.html",
    "href": "posts/Quickest way to add text to ggplot/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/Super Helpful Very Little Tips and Tricks/post.html",
    "href": "posts/Super Helpful Very Little Tips and Tricks/post.html",
    "title": "Super Helpful Very Little Tips and Tricks",
    "section": "",
    "text": "Paste Windows File Directory\nSomething about how annoying the backslashes are etc.\n\nreadClipboard() %&gt;% str_replace_all(\"\\\\\\\\\", \"/\")\n\n [1] \"2021\\t 143,032 \\t 243,176 \" \"2022\\t 148,153 \\t 249,769 \"\n [3] \"2023\\t 152,999 \\t 256,275 \" \"2024\\t 158,001 \\t 263,121 \"\n [5] \"2025\\t 163,048 \\t 270,302 \" \"2026\\t 168,403 \\t 278,048 \"\n [7] \"2027\\t 174,322 \\t 286,863 \" \"2028\\t 179,888 \\t 295,495 \"\n [9] \"2029\\t 185,526 \\t 304,526 \" \"2030\\t 191,171 \\t 313,759 \"\n[11] \"2031\\t 197,013 \\t 323,327 \" \"2032\\t 203,332 \\t 333,890 \"\n[13] \"2033\\t 209,004 \\t 343,690 \" \"2034\\t 214,549 \\t 353,594 \"\n[15] \"2035\\t 219,854 \\t 363,321 \" \"2036\\t 225,278 \\t 373,119 \"\n[17] \"2037\\t 231,106 \\t 384,012 \" \"2038\\t 236,280 \\t 393,893 \"\n[19] \"2039\\t 240,905 \\t 402,916 \" \"2040\\t 245,281 \\t 411,606 \"\n\npaste.path = function() {\n  raw_dir &lt;- readClipboard()\n\n  if(is.null(raw_dir)) {\n    stop(\"Oops, your clipboard appears to be empty\")\n  }\n\n  if(!str_detect(raw_dir, \"\\\\\\\\\")) {\n    stop(stringr::str_glue(\"Hmm it seems you haven't provided a windows file directory path: \\n{format(raw_dir)}\"))\n  }\n\n  str_replace_all(raw_dir, \"\\\\\\\\\", \"/\")\n}"
  },
  {
    "objectID": "ggplot-series.html",
    "href": "ggplot-series.html",
    "title": "Series: Everything I know about ggplot2",
    "section": "",
    "text": "Accurately Viewing and Saving Plots using Camcorder\n\n\n\n\n\n\n\n\n\n\n\n\nNov 12, 2023\n\n\n\n\n\n\n\n\nCreating a ggplot2 theme\n\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\n\n\n\n\n\n\nHighlighting In ggplot\n\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\n\n\n\n\n\n\nCreating a ggplot2 theme\n\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\n\n\n\n\n\nAug 27, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/ggplot2/Quickly highlighting ggplot columns/post.html",
    "href": "posts/ggplot2/Quickly highlighting ggplot columns/post.html",
    "title": "Highlighting In ggplot",
    "section": "",
    "text": "Highlighting specific columns in ggplot is a useful\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/ggplot2/Accurately Viewing and Saving Plots with Camcorder/index.html",
    "href": "posts/ggplot2/Accurately Viewing and Saving Plots with Camcorder/index.html",
    "title": "Accurately Viewing and Saving Plots using Camcorder",
    "section": "",
    "text": "Something I have always struggled with ggplot is after I have spent how many hours tinkering and finessing a plot, I will export it\nTHIS IS A TEST"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Text Pre-Processing\n\n\n\n\n\n\n\n\n\nMar 26, 2024\n\n\n\n\n\n\n\n\nText Vectorisation\n\n\n\n\n\n\n\n\n\nMar 26, 2024\n\n\n\n\n\n\n\n\nTopic Modeling\n\n\n\n\n\n\n\n\n\nMar 26, 2024\n\n\n\n\n\n\n\n\nTopic Modeling in NLP Outline\n\n\n\n\n\n\n\n\n\nJan 19, 2024\n\n\n\n\n\n\n\n\nIntroduction to the Series\n\n\n\n\n\n\n\n\n\nJan 19, 2024\n\n\n\n\n\n\n\n\nStructuring your R Project\n\n\n\n\n\n\n\n\n\nJan 19, 2024\n\n\n\n\n\n\n\n\nCopying Windows File Paths\n\n\n\n\n\n\n\n\n\nJan 12, 2024\n\n\n\n\n\n\n\n\nNamed Lists from Group Maps\n\n\n\n\n\n\n\n\n\nDec 12, 2023\n\n\n\n\n\n\n\n\nAccurately Viewing and Saving Plots using Camcorder\n\n\n\n\n\n\n\n\n\nNov 12, 2023\n\n\n\n\n\n\n\n\nSuper Helpful Very Little Tips and Tricks\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\n\n\n\n\n\n\nCreating a ggplot2 theme\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\n\n\n\n\n\n\nHighlighting In ggplot\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\n\n\n\n\n\n\nCreating a ggplot2 theme\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\n\n\n\nAug 30, 2023\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\n\n\n\nAug 30, 2023\n\n\n\n\n\n\n\n\nWoRkflow\n\n\n\n\n\n\n\n\n\nAug 30, 2023\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\n\n\nAug 27, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/ggplot2/Quickest way to add text to ggplot/index.html",
    "href": "posts/ggplot2/Quickest way to add text to ggplot/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/Group Map Named List/index.html",
    "href": "posts/Group Map Named List/index.html",
    "title": "Named Lists from Group Maps",
    "section": "",
    "text": "The group_map() function is one of my favourite iterative functions within the purrr package. Like the other iterative functions within the purrr-verse, it has been designed to be simple and intuitive.\nIn a nutshell, group_map() applies a function over each group of a grouped dataframe and returns a list containing the function result per group. This is great when we want to…well apply a function on a per group basis.\nAs a quick example, say we want to run a regression to pedict petal length with petal width for each class of species using data in the iris dataset. A group_map() will allow you to do this, and save the outcome in one object so that you can quickly access each regression outcome.\n\nlibrary(dplyr)\nlibrary(purrr)\n\niris_df &lt;- datasets::iris %&gt;% as_tibble()\n\nlm_formula &lt;- formula(\"Petal.Length ~ Petal.Width\")\n\nlm_results &lt;- iris_df %&gt;% \n  group_by(Species) %&gt;% \n  group_map(function(dat, group) {\n    lm(lm_formula, data = dat)\n  })\n\nAnd now we can quickly access our lm results by calling lm_results. Combining this with broom’s tidy in a map() means you can quickly see the key results of each regression.\n\nmap(lm_results, broom::tidy)\n\n[[1]]\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    1.33     0.0600     22.1  7.68e-27\n2 Petal.Width    0.546    0.224       2.44 1.86e- 2\n\n[[2]]\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     1.78     0.284      6.28 9.48e- 8\n2 Petal.Width     1.87     0.212      8.83 1.27e-11\n\n[[3]]\n# A tibble: 2 × 5\n  term        estimate std.error statistic       p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)    4.24      0.561      7.56 0.00000000104\n2 Petal.Width    0.647     0.275      2.36 0.0225       \n\n\nHere you may notice the key issue for which this week’s blog post is designed to address: How can you tell which result corresponds to which group?\nThe solution is reasonably simple: Create a list that contains the lm object and set the element names of the list to be the current group.\n\nlm_results &lt;- iris_df %&gt;% \n  group_by(Species) %&gt;% \n  group_map(function(dat, group) {\n    model &lt;- lm(lm_formula, data = dat)\n    \n    # Place the lm model into a list and set the name to be the current group\n    list(model) %&gt;% \n      setNames(group$Species)\n  })\n\nlm_results\n\n[[1]]\n[[1]]$setosa\n\nCall:\nlm(formula = lm_formula, data = dat)\n\nCoefficients:\n(Intercept)  Petal.Width  \n     1.3276       0.5465  \n\n\n\n[[2]]\n[[2]]$versicolor\n\nCall:\nlm(formula = lm_formula, data = dat)\n\nCoefficients:\n(Intercept)  Petal.Width  \n      1.781        1.869  \n\n\n\n[[3]]\n[[3]]$virginica\n\nCall:\nlm(formula = lm_formula, data = dat)\n\nCoefficients:\n(Intercept)  Petal.Width  \n     4.2407       0.6473  \n\n\nAnd now you can see which lm model corresponds to which species group. However what this has done is added each named list element to the list that is created by group_map(). So while we can see in our output which element of lm_results contains which lm model, we cannot easily access these elements with the $ operator, nor can we quickly iterate over each element like we could do previously.\nThankfully, the solution again is simple: if we use flatten() on our group_map() result, it will remove the nested layers that have in our list and put all the elements in one single layer.\n\nlm_results &lt;- flatten(lm_results)\n\nlm_results\n\n$setosa\n\nCall:\nlm(formula = lm_formula, data = dat)\n\nCoefficients:\n(Intercept)  Petal.Width  \n     1.3276       0.5465  \n\n\n$versicolor\n\nCall:\nlm(formula = lm_formula, data = dat)\n\nCoefficients:\n(Intercept)  Petal.Width  \n      1.781        1.869  \n\n\n$virginica\n\nCall:\nlm(formula = lm_formula, data = dat)\n\nCoefficients:\n(Intercept)  Petal.Width  \n     4.2407       0.6473  \n\n\nAnd now we all of our regression results packaged neatly into one, with each element clearly labelled as so:"
  },
  {
    "objectID": "posts/ggplot2/Setting up a ggplot2 theme/post.html",
    "href": "posts/ggplot2/Setting up a ggplot2 theme/post.html",
    "title": "Creating a ggplot2 theme",
    "section": "",
    "text": "Creating a ggplot2 theme is a cool thing to do."
  },
  {
    "objectID": "posts/ggplot2/Adding Summary Statistics/post.html",
    "href": "posts/ggplot2/Adding Summary Statistics/post.html",
    "title": "Creating a ggplot2 theme",
    "section": "",
    "text": "if(FALSE) {\n\"https://stackoverflow.com/questions/15720545/use-stat-summary-to-annotate-plot-with-number-of-observations\"\n  \n}"
  },
  {
    "objectID": "series/ggplot-series.html",
    "href": "series/ggplot-series.html",
    "title": "Series: Everything I know about ggplot2",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts/workflow/your first r package/first-package.html",
    "href": "posts/workflow/your first r package/first-package.html",
    "title": "What to do with your First R Package",
    "section": "",
    "text": "This is a test"
  },
  {
    "objectID": "series/workflow-series.html",
    "href": "series/workflow-series.html",
    "title": "Series: WoRkflow",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "workflow-series.html",
    "href": "workflow-series.html",
    "title": "Series: WoRkflow",
    "section": "",
    "text": "Introduction to the Series\n\n\n\n\n\n\n\n\n\n\n\n\nJan 19, 2024\n\n\n\n\n\n\n\n\nStructuring your R Project\n\n\n\n\n\n\n\n\n\n\n\n\nJan 19, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/workflow/intro to series/intro.html",
    "href": "posts/workflow/intro to series/intro.html",
    "title": "Introduction to the Series",
    "section": "",
    "text": "This series will focus on my thoughts and methods around structuring R studio projects with little side-quests on things like working with git and documentation.\nIn the five or so years that I have been using R, I have found there to be a surprising lack of resources on this topic (though two resources I have found helpful are this post by David Keyes - notably his take on how to write a README file, and this in depth guide on Reproducible Data Analysis and Reporting by Frank E Harrell Jr) and so I thought I would share what I have learnt (and am still learning!) along my R journey.\nThis series will likely be a perpetual work-in-progress as I continue to learn and refine my approach and I am very much open to feedback on improving the workflow presented here.\nIn brief, some of the topics I expect to cover in the coming weeks include:\n\nSetting up a project template\nHousing the project template in a package to be called when you want to start a new project\nA bit on Renv and reproducible environments\nA bit on saving and loading objects with qs"
  },
  {
    "objectID": "posts/NLP/Outline/outline.html",
    "href": "posts/NLP/Outline/outline.html",
    "title": "Topic Modeling in NLP Outline",
    "section": "",
    "text": "Rather than try and “learn everything I can” about Natural Language Processing, I figured the better method would be to first outline my goals in learning NLP. From here, I can design a program that will enable me to achieve these goals as efficiently as possible.\nIn short then, my main goal for learning NLP is to be able to extract key themes from a collection of documents (known in NLP-speak as a “corpus”). I want to be able to input a corpus into a model, which will then identify and extract themes or topics, both across the corpus as a whole and within documents in the corpus. These themes should be quantified in terms of their importance.\nA quick bit of research has indicated that Topic Modeling is the technique that I need to become familiar with.\nTopic modeling is a statistical model that identifies the unique themes (or “topics”) that are present in a collection of documents. These topics are identified and their importance quantified by the number of similar words that pertain to that topic in a document. The end result is an extraction of the topics in a corpus as well as the balance of topic’s within each document.\nTo understand and utilise topic modeling, there are number of pre-requisite skills that I will need to understand first.\nThroughout this series I will be working with the New Testament from the King James bible, from the package scripturRs."
  },
  {
    "objectID": "posts/NLP/Outline/outline.html#text-pre-processing",
    "href": "posts/NLP/Outline/outline.html#text-pre-processing",
    "title": "Topic Modeling in NLP Outline",
    "section": "Text Pre-Processing",
    "text": "Text Pre-Processing\nClean and pre-process text data to remove noise, irrelevant information and standardise the format for further processing and analysis.\nLearning Outcomes:\n\nTokenisation\nStopword removal\nStemming\nLemmatisation"
  },
  {
    "objectID": "posts/NLP/Outline/outline.html#text-representation",
    "href": "posts/NLP/Outline/outline.html#text-representation",
    "title": "NLP Outline",
    "section": "Text Representation",
    "text": "Text Representation\nAfter pre-processing, the text must be converted into a format that will allow for more complex analysis. This usually involves attributing a quantitative value to the text (either specific words, words within documents or documents within a corpus).\nLearning Outcomes:\n\nBad-of-words model\nTerm Frequency - Inverse Document Frequency (TF-IDF)\nWord embeddings (word2vec, GloVe)\nContextual embeddings (BERT, GPT)"
  },
  {
    "objectID": "posts/NLP/Outline/outline.html#topic-modeling",
    "href": "posts/NLP/Outline/outline.html#topic-modeling",
    "title": "Topic Modeling in NLP Outline",
    "section": "Topic Modeling",
    "text": "Topic Modeling\nOnce the text has been pre-processed and is now in a format that computers and statistical models will understand, we can apply these models to extract the topics that are contained within our text.\nThe most popular model for achieving this seems to be Latent Dirichlet Allocation (LDA)."
  },
  {
    "objectID": "posts/NLP/Text Pre-Processing/text_pre-processing.html",
    "href": "posts/NLP/Text Pre-Processing/text_pre-processing.html",
    "title": "Text Pre-Processing",
    "section": "",
    "text": "library(DiagrammeR)\n#| echo: false\ngrViz(\"\n  digraph {\n  layout = dot\n  node [shape = rectangle,style=filled,fixedsize=False]\n  edge[color=grey,arrowhead=vee]\n  A[label = 'Corpus']\n  B[label = 'Pre-Processing']\n  C[label = 'Tokenisation']\n  D[label = 'Stop Word Removal']\n  E[label = 'Stemming/Lemmatisation']\n\n  A-&gt;B\n  B-&gt;C\n  B-&gt;D\n  B-&gt;E\n\n\n  }\n   \")\nText, like all data, comes in a messy format. Whether that text be a collection of blog posts, newspaper articles, a book, the text must be prepared and organised in a format that will make it more efficient to apply further techniques to it.\nIn this section we cover the key steps involved in text pre-processing:"
  },
  {
    "objectID": "posts/NLP/Text Pre-Processing/text_pre-processing.html#load-packages-and-data",
    "href": "posts/NLP/Text Pre-Processing/text_pre-processing.html#load-packages-and-data",
    "title": "Text Pre-Processing",
    "section": "Load Packages and Data",
    "text": "Load Packages and Data\nMuch of our pre-processing can be completed with the tidytext package.\n\nlibrary(tidytext)\nlibrary(scriptuRs)"
  },
  {
    "objectID": "posts/NLP/Text Pre-Processing/text_pre-processing.html#text-pre-processing",
    "href": "posts/NLP/Text Pre-Processing/text_pre-processing.html#text-pre-processing",
    "title": "Text Pre-Processing",
    "section": "Text Pre-Processing",
    "text": "Text Pre-Processing\nClean and pre-process text data to remove noise, irrelevant information and standardise the format for further processing and analysis.\nLearning Outcomes:\n\nTokenisation\nStopword removal\nStemming\nLemmatisation"
  },
  {
    "objectID": "posts/NLP/Text Pre-Processing/text_pre-processing.html#text-representation",
    "href": "posts/NLP/Text Pre-Processing/text_pre-processing.html#text-representation",
    "title": "Text Pre-Processing",
    "section": "Text Representation",
    "text": "Text Representation\n[Text here]\nLearning Outcomes:\n\nBad-of-words model\nTerm Frequency - Inverse Document Frequency (TF-IDF)\nWord embeddings (word2vec, GloVe)\nContextual embeddings (BERT, GPT)"
  },
  {
    "objectID": "posts/NLP/Text Pre-Processing/text_pre-processing.html#topic-modeling",
    "href": "posts/NLP/Text Pre-Processing/text_pre-processing.html#topic-modeling",
    "title": "Text Pre-Processing",
    "section": "Topic Modeling",
    "text": "Topic Modeling\nIdentify themes or topics present in a collection of documents\nLearning Outcomes:\n\nLatent Dirichlet Allocation (LDA)\n\n\nTopic Modeling\nLearning Outcomes: Identify and extract the underlying themes or topics present in a corpus."
  },
  {
    "objectID": "posts/NLP/Text Pre-Processing/text_pre-processing.html#footnotes",
    "href": "posts/NLP/Text Pre-Processing/text_pre-processing.html#footnotes",
    "title": "Text Pre-Processing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA token is the smallest attribute that you are interested in (e.g., characters, words, sentences)↩︎"
  },
  {
    "objectID": "posts/NLP/Text Pre-Processing/text_pre-processing.html#packages-and-data",
    "href": "posts/NLP/Text Pre-Processing/text_pre-processing.html#packages-and-data",
    "title": "Text Pre-Processing",
    "section": "Packages and Data",
    "text": "Packages and Data\nText pre-processing will be done with the {tidytext} and {textstem} packages. The data-set we are using for text pre-processing is the book of Matthew from the New Testament of the King James Bible. We will access this through the new_testament() function in the {scriptuRs} package.\n\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(textstem)\n\nnewtest_df &lt;- scriptuRs::new_testament %&gt;% tibble()\nmatthew_df &lt;- newtest_df %&gt;% \n  filter(book_title == \"Matthew\")\n\nglimpse(matthew_df)\n\nRows: 1,071\nColumns: 19\n$ volume_id          &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ book_id            &lt;dbl&gt; 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40,…\n$ chapter_id         &lt;dbl&gt; 930, 930, 930, 930, 930, 930, 930, 930, 930, 930, 9…\n$ verse_id           &lt;dbl&gt; 23146, 23147, 23148, 23149, 23150, 23151, 23152, 23…\n$ volume_title       &lt;chr&gt; \"New Testament\", \"New Testament\", \"New Testament\", …\n$ book_title         &lt;chr&gt; \"Matthew\", \"Matthew\", \"Matthew\", \"Matthew\", \"Matthe…\n$ volume_long_title  &lt;chr&gt; \"The New Testament\", \"The New Testament\", \"The New …\n$ book_long_title    &lt;chr&gt; \"The Gospel According to St Matthew\", \"The Gospel A…\n$ volume_subtitle    &lt;chr&gt; \"Of our Lord and Saviour Jesus Christ\", \"Of our Lor…\n$ book_subtitle      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ volume_short_title &lt;chr&gt; \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT…\n$ book_short_title   &lt;chr&gt; \"Matt.\", \"Matt.\", \"Matt.\", \"Matt.\", \"Matt.\", \"Matt.…\n$ volume_lds_url     &lt;chr&gt; \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt…\n$ book_lds_url       &lt;chr&gt; \"matt\", \"matt\", \"matt\", \"matt\", \"matt\", \"matt\", \"ma…\n$ chapter_number     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ verse_number       &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, …\n$ text               &lt;chr&gt; \"THE book of the generation of Jesus Christ, the so…\n$ verse_title        &lt;chr&gt; \"Matthew 1:1\", \"Matthew 1:2\", \"Matthew 1:3\", \"Matth…\n$ verse_short_title  &lt;chr&gt; \"Matt. 1:1\", \"Matt. 1:2\", \"Matt. 1:3\", \"Matt. 1:4\",…\n\nmatthew_df %&gt;% \n  summarise(mean.verse.length = mean(str_count(text, pattern = \"\\\\w+\")),\n            min.verse.length = min(str_count(text, pattern = \"\\\\w+\")),\n            max.verse.length = max(str_count(text, pattern = \"\\\\w+\")))\n\n# A tibble: 1 × 3\n  mean.verse.length min.verse.length max.verse.length\n              &lt;dbl&gt;            &lt;int&gt;            &lt;int&gt;\n1              22.2                6               54\n\n\nEach row of our dataset corresponds to a verse within the book of Matthew. A verse contains on average, 22.2 words."
  },
  {
    "objectID": "posts/NLP/Text Pre-Processing/text_pre-processing.html#tokenisation",
    "href": "posts/NLP/Text Pre-Processing/text_pre-processing.html#tokenisation",
    "title": "Text Pre-Processing",
    "section": "Tokenisation",
    "text": "Tokenisation\nThe unnest_tokens() function from the {tidytext} package quickly extracts the tokens from our dataset. In this case (as in most cases), the token level that we are interested in is “words”.\nGiven “words” is the default token (identified through the token argument, we just need to identify the input (the text column name; in this instance it is simply called text) and the output (the name of the new column that will contain each token; in this instance we will call it “word”)\n\nmatthew_tokens &lt;- matthew_df %&gt;% \n  unnest_tokens(input = text,\n                output = \"word\")\n\n\nglimpse(matthew_tokens)\n\nRows: 23,685\nColumns: 19\n$ volume_id          &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ book_id            &lt;dbl&gt; 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40,…\n$ chapter_id         &lt;dbl&gt; 930, 930, 930, 930, 930, 930, 930, 930, 930, 930, 9…\n$ verse_id           &lt;dbl&gt; 23146, 23146, 23146, 23146, 23146, 23146, 23146, 23…\n$ volume_title       &lt;chr&gt; \"New Testament\", \"New Testament\", \"New Testament\", …\n$ book_title         &lt;chr&gt; \"Matthew\", \"Matthew\", \"Matthew\", \"Matthew\", \"Matthe…\n$ volume_long_title  &lt;chr&gt; \"The New Testament\", \"The New Testament\", \"The New …\n$ book_long_title    &lt;chr&gt; \"The Gospel According to St Matthew\", \"The Gospel A…\n$ volume_subtitle    &lt;chr&gt; \"Of our Lord and Saviour Jesus Christ\", \"Of our Lor…\n$ book_subtitle      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ volume_short_title &lt;chr&gt; \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT…\n$ book_short_title   &lt;chr&gt; \"Matt.\", \"Matt.\", \"Matt.\", \"Matt.\", \"Matt.\", \"Matt.…\n$ volume_lds_url     &lt;chr&gt; \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt…\n$ book_lds_url       &lt;chr&gt; \"matt\", \"matt\", \"matt\", \"matt\", \"matt\", \"matt\", \"ma…\n$ chapter_number     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ verse_number       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, …\n$ verse_title        &lt;chr&gt; \"Matthew 1:1\", \"Matthew 1:1\", \"Matthew 1:1\", \"Matth…\n$ verse_short_title  &lt;chr&gt; \"Matt. 1:1\", \"Matt. 1:1\", \"Matt. 1:1\", \"Matt. 1:1\",…\n$ word               &lt;chr&gt; \"the\", \"book\", \"of\", \"the\", \"generation\", \"of\", \"je…\n\ntoken_count &lt;- matthew_df %&gt;% \n  unnest_tokens(output = \"word\",\n                input = text,\n                token = \"words\") %&gt;% \n  count(word, sort = TRUE)\n\nhead(token_count)\n\n# A tibble: 6 × 2\n  word      n\n  &lt;chr&gt; &lt;int&gt;\n1 and    1552\n2 the    1405\n3 of      672\n4 unto    435\n5 he      412\n6 that    388\n\n\nNow we have a data frame matthew_tokens where each row corresponds to a word (identified in the column word) within each verse.\nNow, we can use count() to see what the most common words are:\n\nmatthew_tokens %&gt;% \n  count(word, sort = TRUE) %&gt;% \n  head()\n\n# A tibble: 6 × 2\n  word      n\n  &lt;chr&gt; &lt;int&gt;\n1 and    1552\n2 the    1405\n3 of      672\n4 unto    435\n5 he      412\n6 that    388\n\n\nAs the output above shows, the most common words don’t give us any insight into what the themes of the text are. Most of the words in this list are common “stop words”. They are words that need to be removed before we can continue with any further processing or analyses.\n\nStop Word Removal\nBefore we can remove any stop words from our dataset, we first need to identify a collection of words that are considered stop words. Thankfully, the {tidytext} package\nWe use the anti_join function and the stop_words dataset from the tidytext package\n\ntoken_count &lt;- matthew_df %&gt;% \n  unnest_tokens(output = \"word\",\n                input = text,\n                token = \"words\") %&gt;% \n  anti_join(stop_words) %&gt;% \n  count(word, sort = TRUE)\n\nJoining with `by = join_by(word)`\n\nhead(token_count, 10)\n\n# A tibble: 10 × 2\n   word          n\n   &lt;chr&gt;     &lt;int&gt;\n 1 ye          263\n 2 jesus       172\n 3 thou        151\n 4 thee        100\n 5 thy          90\n 6 son          78\n 7 heaven       76\n 8 lord         76\n 9 disciples    71\n10 behold       60\n\n\nAnd now we get a much greater understanding as to what themes this text contains. No surprise here that this text.\n\n\nStemming\nwordStem() from the {SnowballC} package.\n\n\nLemmatisation\n\n{udpipe} package - actually can use lemmatize_words from tidytext\n\n\n# library(udpipe)\n\n# udmodel &lt;- udpipe::udpipe_download_model(language = \"english\")\n\nmatthew_token &lt;- matthew_df %&gt;% \n    unnest_tokens(output = \"word\",\n                input = text,\n                token = \"words\") %&gt;% \n  anti_join(stop_words)\n\nJoining with `by = join_by(word)`\n\nmatthew_token %&gt;% \n  mutate(lemma = lemmatize_words(word)) %&gt;% \n  count(lemma, sort = TRUE)\n\n# A tibble: 1,471 × 2\n   lemma        n\n   &lt;chr&gt;    &lt;int&gt;\n 1 ye         263\n 2 jesus      172\n 3 thou       151\n 4 you        100\n 5 thy         90\n 6 son         82\n 7 heaven      78\n 8 lord        76\n 9 disciple    75\n10 behold      62\n# ℹ 1,461 more rows"
  },
  {
    "objectID": "posts/NLP/Outline/outline.html#text-vectorisation",
    "href": "posts/NLP/Outline/outline.html#text-vectorisation",
    "title": "Topic Modeling in NLP Outline",
    "section": "Text Vectorisation",
    "text": "Text Vectorisation\nAfter pre-processing, the text must be converted into numerical format that will allow for more complex analysis.\nLearning Outcomes:\n\nBad-of-words model (including Term Frequency - Inverse Document Frequency [TF-IDF] weightings)\nWord embeddings (word2vec, GloVe)\nContextual embeddings (BERT, GPT)"
  },
  {
    "objectID": "posts/NLP/Text Vectorisation/text-vectorisation.html",
    "href": "posts/NLP/Text Vectorisation/text-vectorisation.html",
    "title": "Text Vectorisation",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidytext)\nlibrary(textstem)\n\n# Input Dataset\nnewtest_df &lt;- scriptuRs::new_testament %&gt;% tibble()\nmatthew_df &lt;- newtest_df %&gt;% \n  filter(book_title == \"Matthew\")\n\n# Identify Stop Words\nmore_stop_words &lt;- tibble(word = c(\"thou\", \"thee\", \"thy\", \"hath\", \"shalt\", \"woe\", \"ye\"), lexicon = \"Az\")\n\nstop_words &lt;- bind_rows(stop_words, more_stop_words)\n\n# Tokenize Data and remove stop words\nmatthew_token_df &lt;- matthew_df %&gt;% \n  unnest_tokens(\n    input = text,\n    output = \"word\"\n  ) %&gt;% \n  anti_join(stop_words)\n\n# Lemmatize words\nmatthew_lemma_df &lt;- matthew_token_df %&gt;% \n  mutate(lemma = lemmatize_words(word))\nIn the previous part of this series, we took a corpus of text (in this case the book of Matthew from the King James Bible) and we applied pre-processing techniques to it to extract the key words in each verse of the book.\nIn this section we will look at different ways to vectorise text to then be used in various Machine Learning models. The reason we do this is because these models can process text if it is linked with a numerical value. The way that we end up representing text will be contingent on the type of model that we want to use, which is dependent of course on the whole purpose of our NLP venture in the first place."
  },
  {
    "objectID": "posts/NLP/Text Vectorisation/text-vectorisation.html#text-vectorisation",
    "href": "posts/NLP/Text Vectorisation/text-vectorisation.html#text-vectorisation",
    "title": "Text Vectorisation",
    "section": "Text Vectorisation",
    "text": "Text Vectorisation\nIn simple terms, text vectorisation is the process of converting text into numerical vectors, which can then be used as input into machine learning models.\nThere are a variety of ways in which text vectorisation can occur. The method chosen should be informed by the desired outcome of your Machine Learning Model. In our example, we are wanting to perform a Topic Analysis. Typically, TF-IDF is sufficient for this approach."
  },
  {
    "objectID": "posts/NLP/Text Vectorisation/text-vectorisation.html#bag-of-words",
    "href": "posts/NLP/Text Vectorisation/text-vectorisation.html#bag-of-words",
    "title": "Text Vectorisation",
    "section": "Bag-of-Words",
    "text": "Bag-of-Words\nMethods that fall under the Bag of Words (BoW) umbrella convert tokens into numerical values corresponding to their frequency. This is done by converting our words into a Term Document or Document Term Matrix (TDM and DTM respectively). These matrices display each word with their corresponding frequency in each document. With TDM, rows represent terms (words or other tokens) and columns represent the documents in the corpus. With DTM the rows represent documents and the the columns represent terms.\nBoW approaches treat each word as a separate feature and ignore the context and order in which they appear. As such these are considered to be the most basic methods of text vectorisation. However, they are less computationally intense as their deep learning counterparts and are sufficient for many NLP use cases.\n\nWord Counts\nWe can use the cast_tdm() function from the {tidytext} package to convert our raw token count into a Term Document Matrix.\n\ntext_corpus &lt;- tibble(text = c(\"This is the first document\",\n                 \"This document is the second document\",\n                 \"And this is the third one\"),\n                 doc = 1:3)\n\ntext_tdm &lt;- text_corpus %&gt;% \n  unnest_tokens(input = text,\n                output = \"word\") %&gt;% \n  count(doc, word) %&gt;% \n  cast_tdm(document = doc,\n           term = word,\n           value = n)\n\nas.matrix(text_tdm)\n\n          Docs\nTerms      1 2 3\n  document 1 2 0\n  first    1 0 0\n  is       1 1 1\n  the      1 1 1\n  this     1 1 1\n  second   0 1 0\n  and      0 0 1\n  one      0 0 1\n  third    0 0 1\n\n\ncast_tdm() has converted our counts into a matrix with the terms as rows and documents as columns and the counts (of terms within each document) as the cell value. The cast_dtm() function does the same thing, but with the rows and columns switched around:\n\ntext_corpus &lt;- tibble(text = c(\"This is the first document\",\n                 \"This document is the second document\",\n                 \"And this is the third one\"),\n                 doc = 1:3)\n\ntext_dtm &lt;- text_corpus %&gt;% \n  unnest_tokens(input = text,\n                output = \"word\") %&gt;% \n  count(doc, word) %&gt;% \n  cast_dtm(document = doc,\n           term = word,\n           value = n)\n\nas.matrix(text_dtm)\n\n    Terms\nDocs document first is the this second and one third\n   1        1     1  1   1    1      0   0   0     0\n   2        2     0  1   1    1      1   0   0     0\n   3        0     0  1   1    1      0   1   1     1\n\n\nFrom what I have seen, the DTM format is more preferable to TDM as it may be more memory-efficient when there is a large ratio of text to documents (which would be the case with most standard text formats).\nIn our example using the book of matthew, the matrix is too large to display. However glimpsing the dtm object confirms that the conversion has worked.\n\nmatthew_dtm &lt;- matthew_lemma_df %&gt;% \n  count(chapter_number, lemma) %&gt;% \n  cast_dtm(document = chapter_number,\n           term = lemma,\n           value = n)\n\nas.matrix(matthew_dtm) %&gt;% glimpse()\n\n num [1:28, 1:1464] 2 0 0 0 0 0 0 0 0 0 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ Docs : chr [1:28] \"1\" \"2\" \"3\" \"4\" ...\n  ..$ Terms: chr [1:1464] \"abia\" \"abiud\" \"abraham\" \"achaz\" ...\n\n\n\n\nTD-IDF\nTerm Document - Inverse Document Frequency is essentially the same as DTM, however instead of raw counts of document terms being used in cells, a weighted count that measures the uniqueness of each term in a document is used. This means that words that are common and unique to a document within a corpus are given a higher score than words that are common across all documents in a corpus. If we break down the term “TF-IDF”, we get a better understanding as to what is going on:\n\nTerm Frequency (TF): Simply the same as what we calculated above. This is the frequency of a term (word) in a given document\nInverse Document Frequency (IDF): If document frequency assesses how common a word is across a corpus, then Inverse Document Frequency assesses how unique a word is across a corpus. Words that are common across many documents will receive a lower score, whereas rare words have a higher score\n\nTF-IDF is simply the product of the TF and IDF scores. A word that is common in one document but rare across all documents will be given a higher score than a word that is common both in one document and across the corpus.\nThe advantage of this method, is that is captures the importance of each word within the context of the corpus. While\n\n\nTerm Frequency-Inverse Document Frequency (TF-IDF)"
  },
  {
    "objectID": "posts/NLP/Text Vectorisation/text-vectorisation.html#topic-modeling",
    "href": "posts/NLP/Text Vectorisation/text-vectorisation.html#topic-modeling",
    "title": "Text Vectorisation",
    "section": "Topic Modeling",
    "text": "Topic Modeling\nIdentify themes or topics present in a collection of documents\nLearning Outcomes:\n\nLatent Dirichlet Allocation (LDA)\n\n\nTopic Modeling\nLearning Outcomes: Identify and extract the underlying themes or topics present in a corpus."
  },
  {
    "objectID": "posts/NLP/Topic Modeling/topic-modeling.html",
    "href": "posts/NLP/Topic Modeling/topic-modeling.html",
    "title": "Topic Modeling",
    "section": "",
    "text": "In the previous part of this series, we took a corpus of text (in this case the book of Matthew from the King James Bible) and we applied pre-processing techniques to it to extract the key words in each verse of the book.\nBefore we can go through the process of topic modeling, we need to represent our tidy text data in a way that makes sense for the topic modeling algorithm (in this case, we are using the popular Latent Dirichlet Allocation [LDA] algorithm to model our data).\nTo build our topic model we first need to convert our text into a Document-Term Matrix (DTM) with TF-IDF weighted frequencies."
  },
  {
    "objectID": "posts/NLP/Topic Modeling/topic-modeling.html#dtm-with-tf-idf",
    "href": "posts/NLP/Topic Modeling/topic-modeling.html#dtm-with-tf-idf",
    "title": "Topic Modeling",
    "section": "DTM with TF-IDF",
    "text": "DTM with TF-IDF\nA Document-Term Matrix (DTM), also known as a term-document matrix, is a numerical representation of text that describes the frequency of tokens (in this case, words) that occur in a collection of documents (in this case, bible verses).\nA DTM can contain binary representations of each word, the actual frequency of each word or a weighted frequency (like the term frequency-inverse document frequency) of each word.\nWe use cast_dtm() from the {tidytext} package along with weightTfIdf() from the {tm} package to create a DTM with TF-IDF weights.\n\n# matthew_matrix &lt;- matthew_lemma_df %&gt;% \n#   count(verse_title, lemma) %&gt;% \n#   cast_dtm(document = verse_title,\n#            term = lemma,\n#            value = n,\n#            weighting = tm::weightTfIdf)\n\n[See claude for interpretation of matrix]"
  }
]