---
title: "Text Vectorisation"
date: "2024-03-26"
categories: [NLP]
editor_options: 
  chunk_output_type: inline
footnotes-hover: true
execute: 
  warning: false
---

```{r}
#| label: pre-process-data
library(tidyverse)
library(tidytext)
library(textstem)

# Input Dataset
newtest_df <- scriptuRs::new_testament %>% tibble()
matthew_df <- newtest_df %>% 
  filter(book_title == "Matthew")

# Identify Stop Words
more_stop_words <- tibble(word = c("thou", "thee", "thy", "hath", "shalt", "woe", "ye"), lexicon = "Az")

stop_words <- bind_rows(stop_words, more_stop_words)

# Tokenize Data and remove stop words
matthew_token_df <- matthew_df %>% 
  unnest_tokens(
    input = text,
    output = "word"
  ) %>% 
  anti_join(stop_words)

# Lemmatize words
matthew_lemma_df <- matthew_token_df %>% 
  mutate(lemma = lemmatize_words(word))
```

```{r}
#| echo: false
library(DiagrammeR)
grViz("
  digraph {
  layout = dot
  node [shape = rectangle,style=filled,fixedsize=False]
  edge[color=grey,arrowhead=vee]
  A0[label = 'Corpus']
  A[label = 'Pre-Processed Text']
  B[label = 'Text Vectorisation', fontname = 'times-bold']
  C[label = 'Bag-of-Words']
  C1[label = 'Word Frequency']
  C2[label = 'TF-IDF']
  E[label = 'Word Embeddings']
  F[label = 'Contextual Embeddings']
  G[label = 'Dimensionality Reduction']
  
  E1[label = 'Word2Vec']
  E2[label = 'GloVe']
  E3[label = 'FastText']
  
  F1[label = 'ELMo']
  F2[label = 'BERT']
  F3[label = 'GPT']

  A0->A
  A->B
  B->C
  B->F
  C->C1
  C->C2
  B->E
  rank=same{B->G} 
  
  
  E -> {E1 E2 E3}
  
  
  F -> {F1 F2 F3}
  


  }
   ")
```

In the previous part of this series, we took a corpus of text (in this case the book of Matthew from the King James Bible) and we applied pre-processing techniques to it to extract the key words in each verse of the book.

In this section we will look at different ways to vectorise text to then be used in various Machine Learning models. The reason we do this is because these models can process text if it is linked with a numerical value. The way that we end up representing text will be contingent on the type of model that we want to use, which is dependent of course on the whole purpose of our NLP venture in the first place.

## Text Vectorisation

In simple terms, text vectorisation is the process of converting text into numerical vectors, which can then be used as input into machine learning models.

There are a variety of ways in which text vectorisation can occur. The method chosen should be informed by the desired outcome of your Machine Learning Model. In our example, we are wanting to perform a Topic Analysis. Typically, TF-IDF is sufficient for this approach.

## Bag-of-Words

Methods that fall under the Bag of Words (BoW) umbrella convert tokens into numerical values corresponding to their frequency. This is done by converting our words into a Term Document or Document Term Matrix (TDM and DTM respectively). These matrices display each word with their corresponding frequency in each document. With TDM, rows represent terms (words or other tokens) and columns represent the documents in the corpus. With DTM the rows represent documents and the the columns represent terms.

BoW approaches treat each word as a separate feature and ignore the context and order in which they appear. As such these are considered to be the most basic methods of text vectorisation. However, they are less computationally intense as their deep learning counterparts and are sufficient for many NLP use cases.

### Word Counts

We can use the <code>cast_tdm()</code> function from the **{tidytext}** package to convert our raw token count into a Term Document Matrix.

```{r}
#| label: TDM-example
text_corpus <- tibble(text = c("This is the first document",
                 "This document is the second document",
                 "And this is the third one"),
                 doc = 1:3)

text_tdm <- text_corpus %>% 
  unnest_tokens(input = text,
                output = "word") %>% 
  count(doc, word) %>% 
  cast_tdm(document = doc,
           term = word,
           value = n)

as.matrix(text_tdm)
```

<code>cast_tdm()</code> has converted our counts into a matrix with the terms as rows and documents as columns and the counts (of terms within each document) as the cell value. The <code>cast_dtm()</code> function does the same thing, but with the rows and columns switched around:

```{r}
#| label: DTM-example
text_corpus <- tibble(text = c("This is the first document",
                 "This document is the second document",
                 "And this is the third one"),
                 doc = 1:3)

text_dtm <- text_corpus %>% 
  unnest_tokens(input = text,
                output = "word") %>% 
  count(doc, word) %>% 
  cast_dtm(document = doc,
           term = word,
           value = n)

as.matrix(text_dtm)
```

From what I gather, the DTM format is preferable to TDM as it may be more memory-efficient when there is a large ratio of text to documents (which would be the case with most standard text formats).

In our example using the book of matthew, the matrix is too large to display. However a glimpse of the <code>dtm</code> object confirms the conversion has worked.

```{r}
#| label: DTM
matthew_dtm <- matthew_lemma_df %>% 
  count(chapter_number, lemma) %>% 
  cast_dtm(document = chapter_number,
           term = lemma,
           value = n)

as.matrix(matthew_dtm) %>% glimpse()
```

### TD-IDF

Term Document - Inverse Document Frequency is essentially the same as DTM, however instead of raw counts of document terms presented in cells, a weighted count that measures the *uniqueness* of each term in a document is used. This means that words that are common and unique to a document within a corpus are given a higher score than words that are common across all documents in a corpus. If we break down the term "TF-IDF", we get a better understanding as to what is going on:

-   **Term Frequency (TF):** Simply the same as what we calculated above. This is the frequency of a term (word) in a given document

-   **Inverse Document Frequency (IDF):** If **document frequency** assesses how common a word is across a corpus, then **Inverse Document Frequency** assesses how unique a word is across a corpus. Words that are common across many documents will receive a lower score, whereas rare words have a higher score

TF-IDF is simply the product of the TF and IDF scores. A word that is common in one document but rare across all documents will be given a higher score than a word that is common both in one document and across the corpus.

The advantage of this method, is that is captures the importance of each word within the context of the corpus.

To create a Document-Term Matrix with TF-IDF weights, we simply need to specify the weight to use in the <code>weighting</code> argument within <code>case_dtm()</code>.

```{r}
#| label: tf-idf
matthew_idf <- matthew_lemma_df %>% 
  count(chapter_number, lemma) %>% 
  cast_dtm(document = chapter_number,
           term = lemma,
           value = n,
           weighting = tm::weightTfIdf)
```

Something we haven't done so far that is important to do now, is to interpret this output from the DTM object, so we will do that now:

```{r}
matthew_idf
```

The first thing to notice is the number of documents and terms. Here we have 28 documents (which in this example are just the chapters within the book) and 1,464 unique terms (words). This doesn't give us much insight but it is good to check as a quality assurance measure.

The next two lines down are the most informative for our ongoing analysis. The line <code>Non-/sparse entries: 4057/36934</code> tells us how many non-zero to zero values (term frequencies) exist in the matrix. The matrix for our 28 documents and 1,464 unique terms contains $28\times1464=40,992$ cells.

Within those cells, 4,057 are **non-zero** **values**, which means that word is present in that document. In the case of un-weighted DTMs, this value can be zero (the word does not appear) or an integer from 1 (denoting the number of times the word appears in that document). In this instance as we are using a TF-IDF weighting, the value will depend on how common the word is presented in the document as well as how common it is in the corpus as a whole.

If 4,057 cells are non-zero values then this means that $40992-4057=36,934$ cells contain **zero values.** Dividing the number of zero values by the total number of cells gives us the Sparsity percentage that we see on the line below. In this instance, within our DTM, 90% of the cells contain zero values. There are a number of implications that stem from having a matrix that is considered "sparse". One key implication is that a sparse matrix can make computational operations less efficient, taking up time and computational resources by running calculations on zero values. To take this into account, we have to perform dimensionality reduction.
