---
title: "Text Pre-Processing"
date: "2024-03-26"
categories: [NLP]
editor_options: 
  chunk_output_type: console
footnotes-hover: true
execute: 
  warning: false
---

```{r}
#| echo: false
library(DiagrammeR)
grViz("
  digraph {
  layout = dot
  node [shape = rectangle,style=filled,fixedsize=False]
  edge[color=grey,arrowhead=vee]
  A[label = 'Corpus']
  B[label = 'Pre-Processing']
  C[label = 'Tokenisation']
  D[label = 'Stop Word Removal']
  E[label = 'Stemming/Lemmatisation']

  A->B
  B->C
  B->D
  B->E


  }
   ")
```

Text, like all data, comes in a messy format. Whether that text be a collection of blog posts, newspaper articles, a book, the text must be prepared and organised in a format that will make it more efficient to apply further techniques to it.

In this section we cover the key steps involved in text pre-processing:

1.  **Tokenisation:** Splitting text into individual tokens. A "token" is the smallest attribute that you are interested in. Usually it is a word, however it could be a character, or sentence and beyond.
2.  **Stop Word Removal:** *Stop Words* are words that are considered insignificant to understanding the meaning of a sentence, but are required to help form a sentence. There is no universal list of stop words, however common stop words include "a", "but", "an", "it", "the", "that" and so on.
3.  **Stemming or Lemmatisation:** Both stemming and lemmatisation are methods to reduce words down to their base or root. For example, "fishing", "fishes", and "fished" will all become just "fish"*.* \<u\>Stemming\</u\> reduces words to their stem just by looking at the suffix of the word, while <u>lemmatisation</u> reduces words to their base or "lemma" by considering the word and the context in which it appears. For example in stemming, the word "caring" would be reduced to the word "car", however in lemmatisation it would be reduced to "care" (the correct form). Given its increased accuracy, Lemmatisation is the tool we shall use.

## Packages and Data

Text pre-processing will be done with the **{tidytext}** and **{textstem}** packages. The data-set we are using for text pre-processing is the book of Matthew from the New Testament of the King James Bible. We will access this through the <code>new_testament()</code> function in the **{scriptuRs}** package.

```{r}
#| label: load-packages
#| warning: false
library(tidyverse)
library(tidytext)
library(textstem)

newtest_df <- scriptuRs::new_testament %>% tibble()
matthew_df <- newtest_df %>% 
  filter(book_title == "Matthew")

glimpse(matthew_df)

matthew_df %>% 
  summarise(mean.verse.length = mean(str_count(text, pattern = "\\w+")),
            min.verse.length = min(str_count(text, pattern = "\\w+")),
            max.verse.length = max(str_count(text, pattern = "\\w+")))
```

Each row of our dataset corresponds to a verse within the book of Matthew. A verse contains on average, 22.2 words.

## Tokenisation

The <code>unnest_tokens()</code> function from the **{tidytext}** package quickly extracts the tokens from our dataset. In this case (as in most cases), the token level that we are interested in is "words".

Given "words" is the default token (identified through the <code>token</code> argument, we just need to identify the **input** (the text column name; in this instance it is simply called `text`) and the **output** (the name of the new column that will contain each token; in this instance we will call it "word")

```{r}
#| label: tokenisation
matthew_token_df <- matthew_df %>% 
  unnest_tokens(input = text,
                output = "word")


glimpse(matthew_token_df)


token_count <- matthew_df %>% 
  unnest_tokens(output = "word",
                input = text,
                token = "words") %>% 
  count(word, sort = TRUE)

head(token_count)

```

Now we have a data frame `matthew_token_df` where each row corresponds to a word (identified in the column `word`) within each verse.

Now, we can use `count()` to see what the most common words are:

```{r}
#| label: count_1
matthew_token_df %>% 
  count(word, sort = TRUE) %>% 
  head()
```

As the output above shows, the most common words don't give us any insight into what the themes of the text are. Most of the words in this list are common "stop words". They are words that need to be removed before we can continue with any further processing or analyses.

### Stop Word Removal

Before we can remove any stop words from our dataset, we first need to identify a collection of words that are considered *stop words.* Thankfully, the **{tidytext}** package contains a `stop_words` data frame that contains commonly used stop words from three different lexicon sources.

Using `anti_join()` we can select only the words in our dataset that do not match any of the stop words.

```{r}
matthew_token_df <- matthew_token_df %>% 
  anti_join(stop_words, by = "word")

matthew_token_df %>% 
  count(word, sort = TRUE) %>% 
  head(10)
```

By removing stop words, we reduced the number of rows in our tokenised dataset from 23,685 to just 7,563! While our list of the most common words is certainly more informative, there are still some words that would be considered stop words, but were missed by our initial anti join. We will quickly fix that up now.

```{r}
#| label: more-stop-words
#| warning: false

more_stop_words <- tibble(word = c("thou", "thee", "thy", "hath", "shalt", "woe"))

matthew_token_df <- matthew_token_df %>% 
  anti_join(more_stop_words)

matthew_token_df %>% 
  count(word, sort = TRUE) %>% 
  head(10)
```

### Lemmatisation

Our final pre-processing stage is to reduce the words down to their base word or "lemma". This will ensure that words with different tenses are captured as one word. For example, in our data-set there are 4 variations of the word "gather". If we do not reduce these down to their lemma "gather", our analyses will look at each word as a completely separate entity.

```{r}
#| label: gather-lemma
matthew_token_df %>% 
  filter(str_detect(word, "gather")) %>% count(word, sort = TRUE)

```

This time we will use `lemmatize_words()` from the **{textstem}** package within a `mutate()` to identify the base of each word and use this base in a new column.

```{r}
#| label: lemmatise-words
matthew_lemma_df <- matthew_token_df %>% 
  mutate(lemma = lemmatize_words(word))
  
matthew_lemma_df %>% 
  filter(str_detect(word, "gather")) %>% 
  count(lemma)

```

Now we can see that the "gather" has been extracted from the words "gathered" and "gathering". As "gathereth" isn't a word that exists in dictionaries these days, it has been ignored. Given it was only used twice in the book, we will leave it as is for now.

Now we are confident that our dataset contains meaningful words that have been reduced down to their base. The next step is to explore different representations of this cleaned data-set.

## Text Representation

\[Text here\]

**Learning Outcomes:**

-   Bad-of-words model

-   Term Frequency - Inverse Document Frequency (TF-IDF)

-   Word embeddings (word2vec, GloVe)

-   Contextual embeddings (BERT, GPT)

## Topic Modeling

Identify themes or topics present in a collection of documents

**Learning Outcomes:**

-   Latent Dirichlet Allocation (LDA)

### Topic Modeling

**Learning Outcomes:** Identify and extract the underlying themes or topics present in a corpus.
