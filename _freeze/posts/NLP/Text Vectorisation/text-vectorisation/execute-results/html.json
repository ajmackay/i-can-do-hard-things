{
  "hash": "5281c8adef26ceb78dda21e4e5bd8194",
  "result": {
    "markdown": "---\ntitle: \"Text Vectorisation\"\ndate: \"2024-03-26\"\ncategories: [NLP]\neditor_options: \n  chunk_output_type: console\nfootnotes-hover: true\nexecute: \n  warning: false\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(textstem)\n\n# Input Dataset\nnewtest_df <- scriptuRs::new_testament %>% tibble()\nmatthew_df <- newtest_df %>% \n  filter(book_title == \"Matthew\")\n\n# Identify Stop Words\nmore_stop_words <- tibble(word = c(\"thou\", \"thee\", \"thy\", \"hath\", \"shalt\", \"woe\", \"ye\"), lexicon = \"Az\")\n\nstop_words <- bind_rows(stop_words, more_stop_words)\n\n# Tokenize Data and remove stop words\nnewtest_token_df <- newtest_df %>% \n  unnest_tokens(\n    input = text,\n    output = \"word\"\n  ) %>% \n  anti_join(stop_words)\n\n# Lemmatize words\nnewtest_lemma_df <- newtest_token_df %>% \n  mutate(lemma = lemmatize_words(word))\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"grViz html-widget html-fill-item\" id=\"htmlwidget-7f83273c783c0803b5ea\" style=\"width:100%;height:464px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-7f83273c783c0803b5ea\">{\"x\":{\"diagram\":\"\\n  digraph {\\n  layout = dot\\n  node [shape = rectangle,style=filled,fixedsize=False]\\n  edge[color=grey,arrowhead=vee]\\n  A0[label = \\\"Corpus\\\"]\\n  A[label = \\\"Pre-Processed Text\\\"]\\n  B[label = \\\"Text Vectorisation\\\", fontname = \\\"times-bold\\\"]\\n  C[label = \\\"Bag-of-Words\\\"]\\n  C1[label = \\\"Word Frequency\\\"]\\n  C2[label = \\\"TF-IDF\\\"]\\n  E[label = \\\"Word Embeddings\\\"]\\n  F[label = \\\"Contextual Embeddings\\\"]\\n  G[label = \\\"Dimensionality Reduction\\\"]\\n  \\n  E1[label = \\\"Word2Vec\\\"]\\n  E2[label = \\\"GloVe\\\"]\\n  E3[label = \\\"FastText\\\"]\\n  \\n  F1[label = \\\"ELMo\\\"]\\n  F2[label = \\\"BERT\\\"]\\n  F3[label = \\\"GPT\\\"]\\n\\n  A0->A\\n  A->B\\n  B->C\\n  B->F\\n  C->C1\\n  C->C2\\n  B->E\\n  rank=same{B->G} \\n  \\n  \\n  E -> {E1 E2 E3}\\n  \\n  \\n  F -> {F1 F2 F3}\\n  \\n\\n\\n  }\\n   \",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\nIn the previous part of this series, we took a corpus of text (in this case the New Testament) and we applied pre-processing techniques to it to extract the key words in each verse of the book.\n\nIn this section we will look at different ways to vectorise text to then be used in various Machine Learning models. The reason we do this is because these models can process text if it is linked with a numerical value. The way that we end up representing text will be contingent on the type of model that we want to use, which is dependent of course on the whole purpose of our NLP venture in the first place.\n\n## Text Vectorisation\n\nIn simple terms, text vectorisation is the process of converting text into numerical vectors, which can then be used as input into machine learning models.\n\nThere are a variety of ways in which text vectorisation can occur. The method chosen should be informed by the desired outcome of your Machine Learning Model. In our example, we are wanting to perform a Topic Analysis. Typically, TF-IDF is sufficient for this approach.\n\n## Bag-of-Words\n\nMethods that fall under the Bag of Words (BoW) umbrella convert tokens into numerical values corresponding to their frequency. This is done by converting our words into a Term Document or Document Term Matrix (TDM and DTM respectively). These matrices display each word with their corresponding frequency in each document. With TDM, rows represent terms (words or other tokens) and columns represent the documents in the corpus. With DTM the rows represent documents and the the columns represent terms.\n\nBoW approaches treat each word as a separate feature and ignore the context and order in which they appear. As such these are considered to be the most basic methods of text vectorisation. However, they are less computationally intense as their deep learning counterparts and are sufficient for many NLP use cases.\n\n### Word Counts\n\nWe can use the <code>cast_tdm()</code> function from the **{tidytext}** package to convert our raw token count into a Term Document Matrix.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntext_corpus <- tibble(text = c(\"This is the first document\",\n                 \"This document is the second document\",\n                 \"And this is the third one\"),\n                 doc = 1:3)\n\ntext_tdm <- text_corpus %>% \n  unnest_tokens(input = text,\n                output = \"word\") %>% \n  count(doc, word) %>% \n  cast_tdm(document = doc,\n           term = word,\n           value = n)\n\nas.matrix(text_tdm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Docs\nTerms      1 2 3\n  document 1 2 0\n  first    1 0 0\n  is       1 1 1\n  the      1 1 1\n  this     1 1 1\n  second   0 1 0\n  and      0 0 1\n  one      0 0 1\n  third    0 0 1\n```\n:::\n:::\n\n\n<code>cast_tdm()</code> has converted our counts into a matrix with the terms as rows and documents as columns and the counts (of terms within each document) as the cell value. The <code>cast_dtm()</code> function does the same thing, but with the rows and columns switched around:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntext_dtm <- text_corpus %>% \n  unnest_tokens(input = text,\n                output = \"word\") %>% \n  count(doc, word) %>% \n  cast_dtm(document = doc,\n           term = word,\n           value = n)\n\nas.matrix(text_dtm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Terms\nDocs document first is the this second and one third\n   1        1     1  1   1    1      0   0   0     0\n   2        2     0  1   1    1      1   0   0     0\n   3        0     0  1   1    1      0   1   1     1\n```\n:::\n:::\n\n\nFrom what I gather, the DTM format is preferable to TDM as it may be more memory-efficient when there is a large ratio of text to documents (which would be the case with most standard text formats).\n\nIn our example using the book of matthew, the matrix is too large to display. However a glimpse of the <code>dtm</code> object confirms the conversion has worked.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnewtest_dtm <- newtest_lemma_df %>% \n  count(verse_id, lemma) %>% \n  cast_dtm(document = verse_id,\n           term = lemma,\n           value = n)\n\nas.matrix(newtest_dtm) %>% glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n num [1:7942, 1:4270] 1 1 0 0 0 0 0 0 0 0 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ Docs : chr [1:7942] \"23146\" \"23147\" \"23148\" \"23149\" ...\n  ..$ Terms: chr [1:4270] \"abraham\" \"book\" \"christ\" \"david\" ...\n```\n:::\n:::\n\n\n### TD-IDF\n\nTerm Document - Inverse Document Frequency is essentially the same as DTM, however instead of raw counts of document terms presented in cells, a weighted count that measures the *uniqueness* of each term in a document is used. This means that words that are common and unique to a document within a corpus are given a higher score than words that are common across all documents in a corpus. If we break down the term \"TF-IDF\", we get a better understanding as to what is going on:\n\n-   **Term Frequency (TF):** Simply the same as what we calculated above. This is the frequency of a term (word) in a given document\n\n-   **Inverse Document Frequency (IDF):** If **document frequency** assesses how common a word is across a corpus, then **Inverse Document Frequency** assesses how unique a word is across a corpus. Words that are common across many documents will receive a lower score, whereas rare words have a higher score\n\nTF-IDF is simply the product of the TF and IDF scores. A word that is common in one document but rare across all documents will be given a higher score than a word that is common both in one document and across the corpus.\n\nThe advantage of this method, is that is captures the importance of each word within the context of the corpus.\n\nTo create a Document-Term Matrix with TF-IDF weights, we simply need to specify the weight to use in the <code>weighting</code> argument within <code>case_dtm()</code>.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnewtest_idf <- newtest_lemma_df %>% \n  count(verse_id, lemma) %>% \n  cast_dtm(document = verse_id,\n           term = lemma,\n           value = n,\n           weighting = tm::weightTfIdf)\n```\n:::\n\n\nSomething we haven't done so far that is important to do now, is to interpret this output from the DTM object, so we will do that now:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnewtest_idf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<<DocumentTermMatrix (documents: 7942, terms: 4270)>>\nNon-/sparse entries: 49688/33862652\nSparsity           : 100%\nMaximal term length: 16\nWeighting          : term frequency - inverse document frequency (normalized) (tf-idf)\n```\n:::\n:::\n\n\nThe first thing to notice is the number of documents and terms. Here we have 7,942 documents (which in this example are the verses within the New Testament) and 4,270 unique terms (words). This doesn't give us much insight but it is good to check as a quality assurance measure.\n\nThe next two lines down are the most informative for our ongoing analysis. The line <code>Non-/sparse entries: 49688/33862652</code> tells us how many non-zero to zero values (term frequencies) exist in the matrix. The matrix for our 7,942 documents and 4,270 unique terms contains $7,942\\times4,270=33912340$ cells.\n\nWithin those cells, 49,688 are **non-zero** **values**, which means that word is present in that document. In the case of un-weighted DTMs, this value can be zero (the word does not appear) or an integer from 1 (denoting the number of times the word appears in that document). In this instance as we are using a TF-IDF weighting, the value will depend on how common the word is presented in the document as well as how common it is in the corpus as a whole.\n\nIf 49,688 cells are non-zero values then this means that $33,912,340-49,688=33,862,652$ cells contain **zero values.** Dividing the number of zero values by the total number of cells gives us the Sparsity percentage that we see on the line below. In this instance, within our DTM, 100% of the cells contain zero values (we know it's not exactly 100% but it is close). There are a number of implications that stem from having a matrix that is considered \"sparse\". One key implication is that a sparse matrix can make computational operations less efficient, taking up time and computational resources by running calculations on zero values. To take this into account, we have to perform dimensionality reduction.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../../site_libs/htmltools-fill-0.5.8.1/fill.css\" rel=\"stylesheet\" />\r\n<script src=\"../../../site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\r\n<script src=\"../../../site_libs/viz-1.8.2/viz.js\"></script>\r\n<link href=\"../../../site_libs/DiagrammeR-styles-0.2/styles.css\" rel=\"stylesheet\" />\r\n<script src=\"../../../site_libs/grViz-binding-1.0.11/grViz.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}