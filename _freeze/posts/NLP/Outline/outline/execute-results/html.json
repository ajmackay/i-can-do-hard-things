{
  "hash": "29d9075203a169b33d95cefe1600a9f5",
  "result": {
    "markdown": "---\ntitle: \"Topic Modeling in NLP Outline\"\ndate: \"2024-01-19\"\ncategories: [NLP]\neditor_options: \n  chunk_output_type: console\nexecute: \n  warning: false\n---\n\n\nRather than try and \"learn everything I can\" about Natural Language Processing, I figured the better method would be to first outline my goals in learning NLP. From here, I can design a program that will enable me to achieve these goals as efficiently as possible.\n\nIn short then, my main goal for learning NLP is to be able to extract key themes from a collection of documents (known in NLP-speak as a \"corpus\"). I want to be able to input a corpus into a model, which will then identify and extract themes or topics, both across the corpus as a whole and within documents in the corpus. These themes should be quantified in terms of their importance.\n\nA quick bit of research has indicated that **Topic Modeling** is the technique that I need to become familiar with.\n\nTopic modeling is a statistical model that identifies the unique themes (or \"topics\") that are present in a collection of documents. These topics are identified and their importance quantified by the number of similar words that pertain to that topic in a document. The end result is an extraction of the topics in a corpus as well as the balance of topic's within each document.\n\nTo understand and utilise topic modeling, there are number of pre-requisite skills that I will need to understand first.\n\nThroughout this series I will be working with the New Testament from the King James bible, from the package <code>scripturRs</code>.\n\n# Syllabus:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(DiagrammeR)\n#| echo: false\n#| include: false\ngrViz(\"\n  digraph {\n  layout = dot\n  node [shape = rectangle,style=filled,fixedsize=False]\n  edge[color=grey,arrowhead=vee]\n  A[label = 'Corpus']\n  B[label = 'Pre-Processing']\n  C[label = 'Tokenisation']\n  D[label = 'Stop Word Removal']\n  E[label = 'Stemming/Lemmatisation']\n\n  F[label = 'Text Vectorisation']\n  G[label = 'Bag-of-Words']\n  J[label = 'Contextual Embeddings']\n  I[label = 'Word Embeddings']\n  \n \n\n  K[label = 'Topic Modeling']\n\n  A->B\n  B->C\n  B->D\n  B->E\n\n  rank=same {B->F}\n  F->G\n  F->I\n  F->J\n  \n \n\n  rank=same {F->K}\n\n  }\n   \")\n```\n\n::: {.cell-output-display}\n```{=html}\n<div class=\"grViz html-widget html-fill-item\" id=\"htmlwidget-f8db7aed6dc65efd670b\" style=\"width:100%;height:464px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-f8db7aed6dc65efd670b\">{\"x\":{\"diagram\":\"\\n  digraph {\\n  layout = dot\\n  node [shape = rectangle,style=filled,fixedsize=False]\\n  edge[color=grey,arrowhead=vee]\\n  A[label = \\\"Corpus\\\"]\\n  B[label = \\\"Pre-Processing\\\"]\\n  C[label = \\\"Tokenisation\\\"]\\n  D[label = \\\"Stop Word Removal\\\"]\\n  E[label = \\\"Stemming/Lemmatisation\\\"]\\n\\n  F[label = \\\"Text Vectorisation\\\"]\\n  G[label = \\\"Bag-of-Words\\\"]\\n  J[label = \\\"Contextual Embeddings\\\"]\\n  I[label = \\\"Word Embeddings\\\"]\\n  \\n \\n\\n  K[label = \\\"Topic Modeling\\\"]\\n\\n  A->B\\n  B->C\\n  B->D\\n  B->E\\n\\n  rank=same {B->F}\\n  F->G\\n  F->I\\n  F->J\\n  \\n \\n\\n  rank=same {F->K}\\n\\n  }\\n   \",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\n## Text Pre-Processing\n\nClean and pre-process text data to remove noise, irrelevant information and standardise the format for further processing and analysis.\n\n**Learning Outcomes:**\n\n-   Tokenisation\n\n-   Stopword removal\n\n-   Stemming\n\n-   Lemmatisation\n\n## Text Vectorisation\n\nAfter pre-processing, the text must be converted into numerical format that will allow for more complex analysis.\n\n**Learning Outcomes:**\n\n-   Bad-of-words model (including Term Frequency - Inverse Document Frequency \\[TF-IDF\\] weightings)\n\n-   Word embeddings (word2vec, GloVe)\n\n-   Contextual embeddings (BERT, GPT)\n\n## Topic Modeling\n\nOnce the text has been pre-processed and is now in a format that computers and statistical models will understand, we can apply these models to extract the topics that are contained within our text.\n\nThe most popular model for achieving this seems to be **Latent Dirichlet Allocation (LDA)**.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../../site_libs/htmltools-fill-0.5.8.1/fill.css\" rel=\"stylesheet\" />\r\n<script src=\"../../../site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\r\n<script src=\"../../../site_libs/viz-1.8.2/viz.js\"></script>\r\n<link href=\"../../../site_libs/DiagrammeR-styles-0.2/styles.css\" rel=\"stylesheet\" />\r\n<script src=\"../../../site_libs/grViz-binding-1.0.11/grViz.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}