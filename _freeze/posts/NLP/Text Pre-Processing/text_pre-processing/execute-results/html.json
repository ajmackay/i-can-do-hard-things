{
  "hash": "d36161a62d1475838d33dd183beda7c4",
  "result": {
    "markdown": "---\ntitle: \"Text Pre-Processing\"\ndate: \"2024-03-26\"\ncategories: [NLP]\neditor_options: \n  chunk_output_type: console\nfootnotes-hover: true\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(DiagrammeR)\n#| echo: false\ngrViz(\"\n  digraph {\n  layout = dot\n  node [shape = rectangle,style=filled,fixedsize=False]\n  edge[color=grey,arrowhead=vee]\n  A[label = 'Corpus']\n  B[label = 'Pre-Processing']\n  C[label = 'Tokenisation']\n  D[label = 'Stop Word Removal']\n  E[label = 'Stemming/Lemmatisation']\n\n  A->B\n  B->C\n  B->D\n  B->E\n\n\n  }\n   \")\n```\n\n::: {.cell-output-display}\n```{=html}\n<div class=\"grViz html-widget html-fill-item\" id=\"htmlwidget-ac6d8dc5ad05f96a4d21\" style=\"width:100%;height:464px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-ac6d8dc5ad05f96a4d21\">{\"x\":{\"diagram\":\"\\n  digraph {\\n  layout = dot\\n  node [shape = rectangle,style=filled,fixedsize=False]\\n  edge[color=grey,arrowhead=vee]\\n  A[label = \\\"Corpus\\\"]\\n  B[label = \\\"Pre-Processing\\\"]\\n  C[label = \\\"Tokenisation\\\"]\\n  D[label = \\\"Stop Word Removal\\\"]\\n  E[label = \\\"Stemming/Lemmatisation\\\"]\\n\\n  A->B\\n  B->C\\n  B->D\\n  B->E\\n\\n\\n  }\\n   \",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\nText, like all data, comes in a messy format. Whether that text be a collection of blog posts, newspaper articles, a book, the text must be prepared and organised in a format that will make it more efficient to apply further techniques to it.\n\nIn this section we cover the key steps involved in text pre-processing:\n\n1.  **Tokenisation:** Splitting text into individual tokens. A \"token\" is the smallest attribute that you are interested in. Usually it is a word, however it could be a character, or sentence and beyond.\n2.  **Stop Word Removal:** *Stop Words* are words that are considered insignificant to understanding the meaning of a sentence, but are required to help form a sentence. There is no universal list of stop words, however common stop words include \"a\", \"but\", \"an\", \"it\", \"the\", \"that\" and so on.\n3.  **Stemming or Lemmatisation:** Both stemming and lemmatisation are methods to reduce words down to their base or root. For example, \"fishing\", \"fishes\", and \"fished\" will all become just \"fish\"*.* \\<u\\>Stemming\\</u\\> reduces words to their stem just by looking at the suffix of the word, while <u>lemmatisation</u> reduces words to their base or \"lemma\" by considering the word and the context in which it appears. For example in stemming, the word \"caring\" would be reduced to the word \"car\", however in lemmatisation it would be reduced to \"care\" (the correct form). Given its increased accuracy, Lemmatisation is the tool we shall use.\n\n## Packages and Data\n\nText pre-processing will be done with the **{tidytext}** and **{textstem}** packages. The data-set we are using for text pre-processing is the book of Matthew from the New Testament of the King James Bible. We will access this through the <code>new_testament()</code> function in the **{scriptuRs}** package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(textstem)\n\nnewtest_df <- scriptuRs::new_testament %>% tibble()\nmatthew_df <- newtest_df %>% \n  filter(book_title == \"Matthew\")\n\nglimpse(matthew_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 1,071\nColumns: 19\n$ volume_id          <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ book_id            <dbl> 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40,…\n$ chapter_id         <dbl> 930, 930, 930, 930, 930, 930, 930, 930, 930, 930, 9…\n$ verse_id           <dbl> 23146, 23147, 23148, 23149, 23150, 23151, 23152, 23…\n$ volume_title       <chr> \"New Testament\", \"New Testament\", \"New Testament\", …\n$ book_title         <chr> \"Matthew\", \"Matthew\", \"Matthew\", \"Matthew\", \"Matthe…\n$ volume_long_title  <chr> \"The New Testament\", \"The New Testament\", \"The New …\n$ book_long_title    <chr> \"The Gospel According to St Matthew\", \"The Gospel A…\n$ volume_subtitle    <chr> \"Of our Lord and Saviour Jesus Christ\", \"Of our Lor…\n$ book_subtitle      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ volume_short_title <chr> \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT…\n$ book_short_title   <chr> \"Matt.\", \"Matt.\", \"Matt.\", \"Matt.\", \"Matt.\", \"Matt.…\n$ volume_lds_url     <chr> \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt…\n$ book_lds_url       <chr> \"matt\", \"matt\", \"matt\", \"matt\", \"matt\", \"matt\", \"ma…\n$ chapter_number     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ verse_number       <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, …\n$ text               <chr> \"THE book of the generation of Jesus Christ, the so…\n$ verse_title        <chr> \"Matthew 1:1\", \"Matthew 1:2\", \"Matthew 1:3\", \"Matth…\n$ verse_short_title  <chr> \"Matt. 1:1\", \"Matt. 1:2\", \"Matt. 1:3\", \"Matt. 1:4\",…\n```\n:::\n\n```{.r .cell-code}\nmatthew_df %>% \n  summarise(mean.verse.length = mean(str_count(text, pattern = \"\\\\w+\")),\n            min.verse.length = min(str_count(text, pattern = \"\\\\w+\")),\n            max.verse.length = max(str_count(text, pattern = \"\\\\w+\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  mean.verse.length min.verse.length max.verse.length\n              <dbl>            <int>            <int>\n1              22.2                6               54\n```\n:::\n:::\n\n\nEach row of our dataset corresponds to a verse within the book of Matthew. A verse contains on average, 22.2 words.\n\n## Tokenisation\n\nThe <code>unnest_tokens()</code> function from the **{tidytext}** package quickly extracts the tokens from our dataset. In this case (as in most cases), the token level that we are interested in is \"words\".\n\nGiven \"words\" is the default token (identified through the <code>token</code> argument, we just need to identify the **input** (the text column name; in this instance it is simply called `text`) and the **output** (the name of the new column that will contain each token; in this instance we will call it \"word\")\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmatthew_tokens <- matthew_df %>% \n  unnest_tokens(input = text,\n                output = \"word\")\n\n\nglimpse(matthew_tokens)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 23,685\nColumns: 19\n$ volume_id          <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ book_id            <dbl> 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40,…\n$ chapter_id         <dbl> 930, 930, 930, 930, 930, 930, 930, 930, 930, 930, 9…\n$ verse_id           <dbl> 23146, 23146, 23146, 23146, 23146, 23146, 23146, 23…\n$ volume_title       <chr> \"New Testament\", \"New Testament\", \"New Testament\", …\n$ book_title         <chr> \"Matthew\", \"Matthew\", \"Matthew\", \"Matthew\", \"Matthe…\n$ volume_long_title  <chr> \"The New Testament\", \"The New Testament\", \"The New …\n$ book_long_title    <chr> \"The Gospel According to St Matthew\", \"The Gospel A…\n$ volume_subtitle    <chr> \"Of our Lord and Saviour Jesus Christ\", \"Of our Lor…\n$ book_subtitle      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ volume_short_title <chr> \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT…\n$ book_short_title   <chr> \"Matt.\", \"Matt.\", \"Matt.\", \"Matt.\", \"Matt.\", \"Matt.…\n$ volume_lds_url     <chr> \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt…\n$ book_lds_url       <chr> \"matt\", \"matt\", \"matt\", \"matt\", \"matt\", \"matt\", \"ma…\n$ chapter_number     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ verse_number       <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, …\n$ verse_title        <chr> \"Matthew 1:1\", \"Matthew 1:1\", \"Matthew 1:1\", \"Matth…\n$ verse_short_title  <chr> \"Matt. 1:1\", \"Matt. 1:1\", \"Matt. 1:1\", \"Matt. 1:1\",…\n$ word               <chr> \"the\", \"book\", \"of\", \"the\", \"generation\", \"of\", \"je…\n```\n:::\n\n```{.r .cell-code}\ntoken_count <- matthew_df %>% \n  unnest_tokens(output = \"word\",\n                input = text,\n                token = \"words\") %>% \n  count(word, sort = TRUE)\n\nhead(token_count)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 2\n  word      n\n  <chr> <int>\n1 and    1552\n2 the    1405\n3 of      672\n4 unto    435\n5 he      412\n6 that    388\n```\n:::\n:::\n\n\nNow we have a data frame `matthew_tokens` where each row corresponds to a word (identified in the column `word`) within each verse.\n\nNow, we can use `count()` to see what the most common words are:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmatthew_tokens %>% \n  count(word, sort = TRUE) %>% \n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 2\n  word      n\n  <chr> <int>\n1 and    1552\n2 the    1405\n3 of      672\n4 unto    435\n5 he      412\n6 that    388\n```\n:::\n:::\n\n\nAs the output above shows, the most common words don't give us any insight into what the themes of the text are. Most of the words in this list are common \"stop words\". They are words that need to be removed before we can continue with any further processing or analyses.\n\n### Stop Word Removal\n\nBefore we can remove any stop words from our dataset, we first need to identify a collection of words that are considered *stop words.* Thankfully, the **{tidytext} package**\n\nWe use the anti_join function and the stop_words dataset from the tidytext package\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntoken_count <- matthew_df %>% \n  unnest_tokens(output = \"word\",\n                input = text,\n                token = \"words\") %>% \n  anti_join(stop_words) %>% \n  count(word, sort = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining with `by = join_by(word)`\n```\n:::\n\n```{.r .cell-code}\nhead(token_count, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 × 2\n   word          n\n   <chr>     <int>\n 1 ye          263\n 2 jesus       172\n 3 thou        151\n 4 thee        100\n 5 thy          90\n 6 son          78\n 7 heaven       76\n 8 lord         76\n 9 disciples    71\n10 behold       60\n```\n:::\n:::\n\n\nAnd now we get a much greater understanding as to what themes this text contains. No surprise here that this text.\n\n### Stemming\n\n<code>wordStem()</code> from the {SnowballC} package.\n\n### Lemmatisation\n\n-   {udpipe} package - actually can use lemmatize_words from tidytext\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# library(udpipe)\n\n# udmodel <- udpipe::udpipe_download_model(language = \"english\")\n\nmatthew_token <- matthew_df %>% \n    unnest_tokens(output = \"word\",\n                input = text,\n                token = \"words\") %>% \n  anti_join(stop_words)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining with `by = join_by(word)`\n```\n:::\n\n```{.r .cell-code}\nmatthew_token %>% \n  mutate(lemma = lemmatize_words(word)) %>% \n  count(lemma, sort = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,471 × 2\n   lemma        n\n   <chr>    <int>\n 1 ye         263\n 2 jesus      172\n 3 thou       151\n 4 you        100\n 5 thy         90\n 6 son         82\n 7 heaven      78\n 8 lord        76\n 9 disciple    75\n10 behold      62\n# ℹ 1,461 more rows\n```\n:::\n:::\n\n\n## Text Representation\n\n\\[Text here\\]\n\n**Learning Outcomes:**\n\n-   Bad-of-words model\n\n-   Term Frequency - Inverse Document Frequency (TF-IDF)\n\n-   Word embeddings (word2vec, GloVe)\n\n-   Contextual embeddings (BERT, GPT)\n\n## Topic Modeling\n\nIdentify themes or topics present in a collection of documents\n\n**Learning Outcomes:**\n\n-   Latent Dirichlet Allocation (LDA)\n\n### Topic Modeling\n\n**Learning Outcomes:** Identify and extract the underlying themes or topics present in a corpus.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../../site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\r\n<script src=\"../../../site_libs/viz-1.8.2/viz.js\"></script>\r\n<link href=\"../../../site_libs/DiagrammeR-styles-0.2/styles.css\" rel=\"stylesheet\" />\r\n<script src=\"../../../site_libs/grViz-binding-1.0.11/grViz.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}